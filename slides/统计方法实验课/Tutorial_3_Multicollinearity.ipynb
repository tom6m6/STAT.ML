{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b6a2e1",
   "metadata": {},
   "source": [
    "# Tutorial: Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193de056",
   "metadata": {},
   "source": [
    "## 实验前的准备\n",
    "\n",
    "本次实验我们载入一些Python的安装包，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f75d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn import preprocessing # Data Preprocessing\n",
    "import statsmodels.api as sm # LSE, Ridge Regression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor # VIF \n",
    "from statsmodels.multivariate.pca import PCA # PCR\n",
    "\n",
    "from jupyterquiz import display_quiz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2c28d",
   "metadata": {},
   "source": [
    "本次实验中，所使用的数据集是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b03a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"./Data/Data_4.csv\")\n",
    "print(Data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117de1c6",
   "metadata": {},
   "source": [
    "我们可以简单统计一下这个数据集中的一些基本信息，如样本量、特征维度，即"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Data.shape[1]-1\n",
    "n = Data.shape[0]\n",
    "\n",
    "print(\"The number of features is\",p)\n",
    "print(\"The sample size is\",n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c74692",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "本次数据来自于1986年至1987年间美国职业棒球大联盟。这个数据记录了263名大联盟的选手所采集的收入数据及其历史棒球运动的表现。\n",
    "\n",
    "## 数据\n",
    "在本次数据中，共有17个变量（1个响应变量与16个自变量），具体如下表所示：\n",
    "<table width = c(100,800,700), center = True>\n",
    "    <tr>\n",
    "        <td> 变量名                  </td>\n",
    "        <td> 英文变量含义                 </td>\n",
    "        <td> 中文变量含义                 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> AtBat                          </td>\n",
    "        <td> Number of times at bat in 1986 </td>\n",
    "        <td> 1986年间轮到击球的次数            </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Hits                         </td>\n",
    "        <td> Number of hits in 1986       </td>\n",
    "        <td> 1986年间安打的总和           </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> HmRun                         </td>\n",
    "        <td> Number of home runs in 1986   </td>\n",
    "        <td> 1986年间全垒打的总和             </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td> Runs                              </td>\n",
    "        <td> Number of runs in 1986            </td>\n",
    "        <td> 1986年间得分的总和                   </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> RBI                               </td>\n",
    "        <td> Number of runs batted in in 1986  </td>\n",
    "        <td> 1986年间得分打的总和                   </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Walks                               </td>\n",
    "        <td> Number of walks in 1986  </td>\n",
    "        <td> 1986年间击球手被保送的次数                   </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Years                               </td>\n",
    "        <td> Number of years in the major leagues  </td>\n",
    "        <td> 在大联盟的年限                  </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td> CAtBat                               </td>\n",
    "        <td> Number of times at bat during his career  </td>\n",
    "        <td> 在其棒球职业生涯中轮到击球的次数               </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> CHits                              </td>\n",
    "        <td> Number of hits during his career  </td>\n",
    "        <td> 在其棒球职业生涯中安打的总和           </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> CHmRun                              </td>\n",
    "        <td> Number of home runs during his career  </td>\n",
    "        <td> 在其棒球职业生涯中全垒打的总和              </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> CRuns                              </td>\n",
    "        <td> Number of runs during his career  </td>\n",
    "        <td> 在其棒球职业生涯中得分的总和                 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> CRBI                              </td>\n",
    "        <td> Number of runs batted in during his career  </td>\n",
    "        <td> 在其棒球职业生涯中得分打的总和                 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> CWalks                              </td>\n",
    "        <td> Number of walks during his career  </td>\n",
    "        <td> 在其棒球职业生涯中击球手被保送的次数                 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> PutOuts                              </td>\n",
    "        <td> Number of put outs in 1986  </td>\n",
    "        <td> 在1986年间接杀的次数                 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Assists                             </td>\n",
    "        <td> Number of assists in 1986  </td>\n",
    "        <td> 在1986年间助杀的次数                 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Errors                            </td>\n",
    "        <td> Number of errors in 1986          </td>\n",
    "        <td> 在1986年间防守时的失误次数                 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Salary                            </td>\n",
    "        <td> 1987 annual salary on opening day in thousands of dollars          </td>\n",
    "        <td> 在1987年开业时年薪（单位：千美元）         </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "我们想了解棒球运动员的收入与其运动表现的关系。因此，Salary是本次模型中的标签（或响应变量），其余16个变量为特征（或自变量）。这个数据集中涉及棒球比赛中的专有名词，同学们可以课后参考视频[1]。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb9e21",
   "metadata": {},
   "source": [
    "## 任务\n",
    "\n",
    "本次实验中，我们需要解决以下问题：\n",
    "\n",
    "1. 如何判断特征数据中是否存在多重共线性？\n",
    "2. 如何构建岭回归模型？\n",
    "3. 如何构建主成分回归模型？\n",
    "4. 从预测的角度而言，普通线性回归模型、岭回归模型以及主成分回归模型，哪个模型更优？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50631e",
   "metadata": {},
   "source": [
    "## 解决方案\n",
    "\n",
    "### Task 0：数据预处理\n",
    "\n",
    "在评价模型时，倘若我们采用同一个数据集进行构建模型与评价模型，往往会造成**过拟合**的情形。为避免这个问题，我们会提前将数据集拆分为两个部分，一部分数据被称为训练集，用于模型训练。另一个部分为测试集，用于评价模型。\n",
    "\n",
    "留出法是划分数据集的一种常见方法。在本次实验中，我们保留80%的数据用于训练模型，而20%的数据用于评价模型。在划分训练集和测试集时，我们采用随机采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 210 # 80% instances used for training\n",
    "n_test = n-n_train\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "index = range(0,n)\n",
    "index_selected = random.sample(index,n_train)\n",
    "index_selected.sort()\n",
    "\n",
    "Data_train = Data.loc[index_selected]\n",
    "Data_test = Data.drop(index = index_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3199c",
   "metadata": {},
   "source": [
    "用于建模的数据集为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65edb4df",
   "metadata": {},
   "source": [
    "用于评价的数据集为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96266c39",
   "metadata": {},
   "source": [
    "除了划分训练集和测试集之外，我们还需要将数据进行预处理。在本次实验中，我们将特征，即自变量，进行**标准化**，而将标签，即响应变量，进行中心化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing\n",
    "X_train = Data_train.drop(columns = ['Salary'],axis = 1)\n",
    "Y_train = Data_train.Salary\n",
    "X_test = Data_test.drop(columns = [\"Salary\"],axis=1)\n",
    "Y_test = Data_test.Salary\n",
    "\n",
    "X_train_standardized = preprocessing.scale(X_train, with_mean = True, with_std=True)/np.sqrt(n_train)\n",
    "Y_train_centered = preprocessing.scale(Y_train, with_mean = True, with_std=False)\n",
    "Y_train_mean = np.average(Y_train)\n",
    "X_test_standardized = preprocessing.scale(X_test, with_mean = True, with_std=True)/np.sqrt(n_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4c239",
   "metadata": {},
   "source": [
    "我们可以建立一个线性回归模型，即"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05788683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_withintercept = sm.add_constant(X_train)\n",
    "model = sm.OLS(Y_train,X_train_withintercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494249ee",
   "metadata": {},
   "source": [
    "在上述结论中，我们发现：\n",
    "1. 有一些特征前的回归系数与专业认知不一致；比如：HmRun。\n",
    "2. 在第二个注意中，Python也表明需要注意多重共线性的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea31bde9",
   "metadata": {},
   "source": [
    "### Task 1:  如何判断特征数据中是否存在多重共线性？\n",
    "\n",
    "多重共线性是严重影响线性回归模型效果的原因之一。倘若数据中存在多重共线性，自变量之间存在接近线性的相关性。虽然多重共线性并不违反线性回归模型的基本假设，但是多重共线性会影响最小二乘估计的效果。\n",
    "\n",
    "一方面，我们可以先关注一下特征之间自身的相关性，并绘制热力图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcorr = X_train.corr(method = 'pearson')\n",
    "tick_ = (np.arange(0,13,2)-2)/10\n",
    "dict_ = {\"orientation\":'vertical',\n",
    "         'label':'Correlation Coefficients',\n",
    "         \"drawedges\":False,\n",
    "         \"ticklocation\":\"top\",\n",
    "         \"extend\":\"min\",\n",
    "         \"alpha\":0.8,\n",
    "         \"cmap\":\"cmap\",\n",
    "         \"ticks\":tick_}\n",
    "sns.heatmap(pcorr,cbar_kws= dict_,center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921652a",
   "metadata": {},
   "source": [
    "从上图中可以发现，颜色越浅的特征之间相关性越强，而颜色越深的特征之间相关性越弱。很容易发现AtBat与Hits，CAtBat与CHits，Assists与Errors都有比较强的相关性。从相关阵可以初步判断出数据之间可能出现**多重共线性**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6b7b8",
   "metadata": {},
   "source": [
    "另一方面，在实际中，我们通常根据方差扩大因子和特征值两类指标来判断数据是否存在多重共线性。\n",
    "\n",
    "#### 方差扩大因子法\n",
    "\n",
    "方差扩大因子法中，一般我们需要设置一个临界值，在本次实验中我们设定为5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50395d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_cv = 5 # The criterion value of VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccda22",
   "metadata": {},
   "source": [
    "方差扩大因子的计算方法：\n",
    "假定特征数据已经经过标准化，现数据有\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n",
    "\\vdots & \\vdots &  & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{np}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "方差扩大因子指的是$(X'X)^{-1}$的对角线元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fea3ad",
   "metadata": {},
   "source": [
    "1. 根据方差扩大因子的定义，我们可以通过矩阵计算来确定方差扩大因子，如方法一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method One\n",
    "C = np.linalg.inv(np.dot(X_train_standardized.T,X_train_standardized))\n",
    "VIF_1 = np.diag(C)\n",
    "print(\"The VIF's are\", np.round(VIF_1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0679e18",
   "metadata": {},
   "source": [
    "2. 在Python中statsmodel包中自带函数variance_inflation_factor也可以直接用于计算方差扩大因子，如方法二。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method Two\n",
    "VIF_2 = [variance_inflation_factor(X_train_standardized[:,0:p], i) for i in range(p)]\n",
    "print(\"The VIF's are\", np.round(VIF_2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a10d7",
   "metadata": {},
   "source": [
    "方差扩大因子的命名来源是基于方法一。但是，问题是：为什么方差扩大因子能够度量特征存在多重共线性？\n",
    "\n",
    "举例来说，以$x_j$作为响应变量，其他特征$x_1,x_2,\\cdots,x_{j-1},x_{j+1},\\cdots,x_{p}$作为自变量，构建线性回归模型即\n",
    "$$\n",
    "x_j = \\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2 + \\cdots +\\alpha_{j-1}x_{j-1} + \\alpha_{j+1}x_{j+1} + \\cdots + \\alpha_p x_p +\\epsilon\n",
    "$$\n",
    "\n",
    "如果这个特征能够被其他特征近似看作其他特征的线性组合，那么我们称这个现象为**多重共线性**。基于这个想法，如果上述回归模型的决定系数$R^2$很接近于1，那么这个特征很有可能看作其他特征的线性组合。因此，多重共线性VIF还有一种计算方法，即\n",
    "$$\n",
    "\\text{VIF}_j = \\frac{1}{1-R_j^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36915af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method Three\n",
    "X_1 = X_train_standardized[:,0]\n",
    "X_others = X_train_standardized[:,1:p]\n",
    "model_vif = sm.OLS(X_1,X_others).fit()\n",
    "X_1_VIF = 1/(1-model_vif.rsquared)\n",
    "print(\"The VIF of the first feature is\", np.round(X_1_VIF,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b991f",
   "metadata": {},
   "source": [
    "方差扩大因子的评价方式：如果方差扩大因子越大，那么可以认为多重共线性是严重的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce9757",
   "metadata": {},
   "source": [
    "可以发现，在计算有些指标的时候，需要调用Python中线性回归模型的结果。我们这里补充一些线性回归模型中常见需要调用的函数，具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 补充内容：\n",
    "model_vif.summary()\n",
    "## 建模类\n",
    "# model_vif.params # 回归系数(向量，p)\n",
    "# model_vif.bse # 回归系数的标准误\n",
    "# model_vif.tvalues # 回归系数的t统计量（向量，p）\n",
    "# model_vif.pvalues # 回归系数的t检验对应的p值（向量，p）\n",
    "# model_vif.fittedvalues # 回归模型的拟合值（向量，n）\n",
    "# model_vif.resid # 回归模型的残差（向量，n）\n",
    "\n",
    "## 评价类\n",
    "# model_vif.fvalue # 回归模型的F检验统计量\n",
    "# model_vif.f_pvalue # 回归模型的F检验对应的p值\n",
    "# model_vif.rsquared # 回归模型的决定系数R2\n",
    "# model_vif.rsquared_adj # 回归模型修正后的决定系数 Adj. R2\n",
    "# model_vif.ess # 回归平方和\n",
    "# model_vif.ssr # 残差平方和\n",
    "# model_vif.aic # AIC\n",
    "# model_vif.bic # BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6b786",
   "metadata": {},
   "source": [
    "#### 特征值方法\n",
    "\n",
    "我们可以计算$\\mathbf{X}'\\mathbf{X}$的特征值来判断数据多重共线性的程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.dot(X_train_standardized.T,X_train_standardized)\n",
    "W, V = np.linalg.eig(R)\n",
    "W = -1* np.sort(-1*W)\n",
    "W_diag = np.diag(W)\n",
    "V = V.T\n",
    "print(\"The eigen values are\", np.round(W,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b48534",
   "metadata": {},
   "source": [
    "这里特别需要说明一下，np.linalg.eig函数所输出特征值并不是按从大到小的顺序进行排列的。\n",
    "\n",
    "基于所计算的特征值，我们进一步需要计算条件数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Condition_Number = np.sqrt(np.max(W)/np.min(W))\n",
    "print(\"The condition number is\",np.round(Condition_Number,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28d138",
   "metadata": {},
   "source": [
    "可以发现：所计算出的条件数也是比较大的。这里也可以预先设置临界值进行判断。\n",
    "\n",
    "同时，我们也可以重新运行线性回归模型，查看所计算的条件数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ols = sm.OLS(Y_train_centered,X_train_standardized).fit()\n",
    "print(\"The condition number is\",round(model_ols.condition_number,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455bc2c",
   "metadata": {},
   "source": [
    "### Task 2：如何构建岭回归模型？\n",
    "\n",
    "岭回归估计为\n",
    "$$\n",
    "\\hat{\\mathbf{\\beta}}(k) = (\\mathbf{X}'\\mathbf{X} + k \\mathbf{I})^{-1} \\mathbf{X}' \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "在Python的statmodels.api包中OLS.fit_regularized可以实现，具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression\n",
    "alpha  = 0.1 # A hyperparameter in Ridge Regression (equivalently but not equally, k in slides)\n",
    "model_rr = sm.OLS(Y_train_centered,X_train_standardized).fit_regularized(L1_wt=0,alpha = alpha)\n",
    "print(\"The parameters in the ridge regression are \",np.round(model_rr.params,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c11c8",
   "metadata": {},
   "source": [
    "在上述代码中，有一个超参数$\\alpha$，与我们在介绍岭回归中的$k$并不是同一个参数，但是与$k$是等价的。这是因为这个函数本身是为了解决Elastic Net问题。在Elastic Net中，最优化问题是\n",
    "$$\n",
    "0.5* RSS/n +\\alpha *((1-L1_{wt})*|\\mathbf{\\beta}|_2^2/2 + L1_{wt} * |\\mathbf{\\beta}|_1)\n",
    "$$\n",
    "其中，\n",
    "- RSS：原本的偏差平方和；\n",
    "- n：样本量；\n",
    "- $|\\cdot|_1,|\\cdot|_2$：$L_1$和$L_2$范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be80d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(\"./Question/T5_Q1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd72bc",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "<details><summary>Q1【点击提示】</summary>\n",
    "    \n",
    "   请运行一下code来进行验证。\n",
    "<code>\n",
    "k = alpha * n_train # The relation between alpha and k\n",
    "print(\"The parameters in the ridge regression are \", np.around(np.linalg.inv(X_train_standardized.T @ X_train_standardized + k * np.eye(p)) @ X_train_standardized.T @ Y_train_centered,2))\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_num = 100\n",
    "alpha_vec = np.array(list(range(0,alpha_num)))/alpha_num/1000\n",
    "RR_coef = []\n",
    "RR_vif = []\n",
    "\n",
    "for alpha in alpha_vec:\n",
    "    rr_model = sm.OLS(Y_train_centered,X_train_standardized).fit_regularized(L1_wt=0,alpha = alpha)\n",
    "    rr_coef = rr_model.params\n",
    "    RR_coef.append(rr_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e4264",
   "metadata": {},
   "source": [
    "如何确定最优的超参数？这是岭回归模型中一个重要的问题。\n",
    "\n",
    "我们可以绘制岭迹图，具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(p):\n",
    "    plt.plot(alpha_vec, np.array(RR_coef)[:,i],label = \"X%d\"%(i+1))\n",
    "\n",
    "plt.axis([-0.00005,0.0008,-6000,8000])    \n",
    "plt.legend(loc=\"right\",bbox_to_anchor=(1.25,0.5),borderaxespad=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b459b7b",
   "metadata": {},
   "source": [
    "同时，我们可以通过计算不同k（alpha）下的VIF值，当VIF符合临界值条件，且为最小的k（alpha），就是我们所选择的超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d348612",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_k = []\n",
    "for alpha in alpha_vec:\n",
    "    k = alpha * n_train\n",
    "    c_k = np.linalg.inv(X_train_standardized.T @ X_train_standardized + k * np.eye(p)) @ X_train_standardized.T @ X_train_standardized @ np.linalg.inv(X_train_standardized.T @ X_train_standardized + k * np.eye(p))\n",
    "    C_k.append(np.mean(np.diag(c_k)))\n",
    "\n",
    "alpha_best = alpha_vec[np.min(np.where(np.array(C_k) < vif_cv))]\n",
    "print(\"The chosen value of alpha is\", alpha_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe93c1",
   "metadata": {},
   "source": [
    "我们重新构建了在最优的k（或者alpha）时的岭回归估计，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rr_best = sm.OLS(Y_train_centered,X_train_standardized).fit_regularized(L1_wt=0,alpha = alpha_best)\n",
    "model_rr_best_coef = model_rr_best.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9ad4f",
   "metadata": {},
   "source": [
    "这里我们得到的估计，记为$\\hat{\\mathbf{\\beta}}^{\\text{rr}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43332e",
   "metadata": {},
   "source": [
    "### Task 3：如何构建主成分回归模型？\n",
    "\n",
    "在PCR模型中，第一步需要构建特征的主成分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca = PCA(X_train_standardized,standardize = False, demean = True)\n",
    "model_pca_cr = model_pca.eigenvals # contribution rate of each component\n",
    "print(\"The percentages of total variance are\", np.around(model_pca_cr,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063a810",
   "metadata": {},
   "source": [
    "以上计算的就是$\\mathbf{X}'\\mathbf{X}$的特征值，也被称为每个主成分的贡献率。通过以下代码可以进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_Mat = X_train_standardized.T @ X_train_standardized\n",
    "Lambda, V = np.linalg.eig(Corr_Mat)\n",
    "Lambda = sorted(Lambda,reverse = True)\n",
    "print(\"The eigen values of X'X are \", np.around(Lambda,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e0f7f",
   "metadata": {},
   "source": [
    "和岭回归模型中的超参数k（或alpha）一样，主成分分析中主成分个数也是一个超参数。如何选择主成分的个数也是一个重要的问题。\n",
    "\n",
    "在主成分分析中，我们可以绘制scree plot来作出主成分个数的主观判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model_pca.plot_scree(log_scale = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5646e9",
   "metadata": {},
   "source": [
    "除此之外，我们还可以计算每增加一个主成分后，对信息**增量**是多少？我们可以计算累积贡献率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d53c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca_ccr = np.cumsum(model_pca_cr)/p # cummulative contribution rate\n",
    "print(\"The cummulative percentages of total variance are\", np.around(model_pca_ccr,4))\n",
    "\n",
    "fig = model_pca.plot_scree(log_scale = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2523d",
   "metadata": {},
   "source": [
    "我们选取主成分时需要其累积贡献率达到85%，即"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cv = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pca = np.min(np.where(model_pca_ccr > pca_cv))\n",
    "print(\"The appropriate number of component is\", (num_pca+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed96107",
   "metadata": {},
   "source": [
    "在确定主成分个数后，我们构建**新**特征（即主成分）代替原始特征，并拟合回归模型后。在通过线性变换，来估计主成分回归中的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca_selected = PCA(X_train_standardized,  standardize = False,  demean = True)\n",
    "V = model_pca_selected.loadings\n",
    "Z_train_standardized= X_train_standardized @ V[:,0:(num_pca+1)]\n",
    "model_pcr = sm.OLS(Y_train_centered, Z_train_standardized).fit()\n",
    "model_pcr_coef = V.T @ np.pad(model_pcr.params,(0,p-(num_pca+1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a4a14",
   "metadata": {},
   "source": [
    "这里我们得到的估计，记为$\\hat{\\mathbf{\\beta}}^{\\text{prc}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597a06d",
   "metadata": {},
   "source": [
    "### Task 4：比较三个模型预测结果\n",
    "\n",
    "由于这个问题是一个回归问题，我们采用RMSE来作为衡量指标，即\n",
    "$$\n",
    "RMSE = \\sqrt{n^{-1}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73fe77",
   "metadata": {},
   "source": [
    "第一，我们先构建线性回归模型。对于每一个数据$(\\mathbf{x}_i,y_i)$，线性回归模型的预测结果为\n",
    "$$\n",
    "y_i^{\\text{ols}} = \\hat{\\beta}_0 + \\mathbf{x}_i \\hat{\\mathbf{\\beta}}^{\\text{ols}}\n",
    "$$\n",
    "这里\n",
    "- $\\hat{\\beta}_0$采用训练集中响应变量的样本均值来估计；\n",
    "- $\\hat{\\mathbf{\\beta}}^{\\text{ols}}$采用最小二乘估计；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0898c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ols_coef = model_ols.params \n",
    "Y_pred_ols = Y_train_mean + X_test_standardized @ model_ols_coef\n",
    "ols_rmse = np.sqrt(np.mean((Y_pred_ols-Y_test)**2))\n",
    "print(\"The RMSE in the ordinary regression is\", round(ols_rmse,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ee412",
   "metadata": {},
   "source": [
    "第二，我们再构建岭回归模型。对于每一个数据$(\\mathbf{x}_i,y_i)$，岭回归模型的预测结果为\n",
    "$$\n",
    "y_i^{\\text{rr}} = \\hat{\\beta}_0 + \\mathbf{x}_i \\hat{\\mathbf{\\beta}}^{\\text{rr}}\n",
    "$$\n",
    "这里\n",
    "- $\\hat{\\beta}_0$采用训练集中响应变量的样本均值来估计；\n",
    "- $\\hat{\\mathbf{\\beta}}^{\\text{rr}}$采用岭回归估计；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rr = Y_train_mean + X_test_standardized @ model_rr_best_coef\n",
    "rr_rmse = np.sqrt(np.mean((Y_pred_rr-Y_test)**2))\n",
    "\n",
    "print(\"The RMSE in the ridge regression is\", round(rr_rmse,4))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b9640",
   "metadata": {},
   "source": [
    "第三，我们最后构建主成分回归模型。对于每一个数据$(\\mathbf{x}_i,y_i)$，主成分回归模型的预测结果为\n",
    "$$\n",
    "y_i^{\\text{pcr}} = \\hat{\\beta}_0 + \\mathbf{x}_i \\hat{\\mathbf{\\beta}}^{\\text{pcr}}\n",
    "$$\n",
    "这里\n",
    "- $\\hat{\\beta}_0$采用训练集中响应变量的样本均值来估计；\n",
    "- $\\hat{\\mathbf{\\beta}}^{\\text{pcrr}}$采用主成分回归估计；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511abe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_pcr = Y_train_mean + X_test_standardized @ model_pcr_coef\n",
    "pcr_rmse = np.sqrt(np.mean((Y_pred_pcr-Y_test)**2))\n",
    "\n",
    "print(\"The RMSE in the principal component regression is\", round(pcr_rmse,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bce265",
   "metadata": {},
   "source": [
    "从预测的角度来看，岭回归模型和主成分回归模型均优于普通线性回归模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a615d",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] 棒球规则速成！从棒球小白到棒球通你只需要点开这个视频【棒球101合集】网址：https://www.bilibili.com/video/BV1Fa411P7P7/?spm_id_from=333.337.search-card.all.click&vd_source=1c65b0c7684a3149a7392f98602a49b4\n",
    "\n",
    "[2] Izenman A J. Modern multivariate statistical techniques[J]. Regression, classification and manifold learning, 2008, 10: 978-0.\n",
    "\n",
    "[3] Zou H, Hastie T. Regularization and variable selection via the elastic net[J]. Journal of the royal statistical society: series B (statistical methodology), 2005, 67(2): 301-320."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
